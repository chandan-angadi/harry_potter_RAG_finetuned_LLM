{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§™â€â™‚ï¸ Welcome to Fine-Tuning with LLaMA3.2 ðŸ¦™\n",
    "\n",
    "In this notebook, we'll explore how to fine-tune the **LLaMA3.2** language model using **parameter-efficient fine-tuning** techniques like LoRA and PEFT. This notebook will also demonstrate how to create a retrieval-augmented generation (RAG) system and evaluate the fine-tuned models using popular NLP metrics.\n",
    "\n",
    "Let's embark on a magical journey, much like Harry Potter's adventures! ðŸŽ©âœ¨\n",
    "\n",
    "![Harry Potter](https://upload.wikimedia.org/wikipedia/en/7/7a/Harry_Potter_and_the_Philosopher%27s_Stone_banner.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Required Packages\n",
    "# Code Cell:\n",
    "#!pip install transformers torch peft datasets scikit-learn nltk rouge-score meteor-score langchain faiss-cpu matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries\n",
    "Below, we import the libraries required for data processing, model fine-tuning, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from torch.utils.data import DataLoader\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Constants\n",
    "Here, we specify paths and configuration parameters for the model, datasets, and output directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"models\"\n",
    "TRAINING_DATA_PATH = \"data/train.jsonl\"\n",
    "EVALUATION_DATA_PATH = \"data/test.jsonl\"\n",
    "MODEL = \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Utility Functions\n",
    "We define helper functions for text preprocessing, data saving, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "def preprocess_text(page_text):\n",
    "    \"\"\"Preprocess text by removing noise and irrelevant content.\"\"\"\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", page_text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Replace multiple spaces with a single space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load Training and Evaluation Data\n",
    "We load the training and evaluation datasets for fine-tuning and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load data from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of data entries\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "def prepare_data(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Prepare training data in Ollama's expected format.\n",
    "    \n",
    "    Args:\n",
    "        file_path: str\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Formatted training data for Ollama\n",
    "    \"\"\"\n",
    "    data = load_jsonl_data(file_path=file_path)\n",
    "    ollama_data = []\n",
    "    for item in data:\n",
    "        ollama_data.append({\n",
    "            'prompt': preprocess_text(item['question']),\n",
    "            'response': preprocess_text(item['answer'])\n",
    "        })\n",
    "    return ollama_data\n",
    "\n",
    "training_dataset = prepare_data(TRAINING_DATA_PATH)\n",
    "evaluation_dataset = prepare_data(EVALUATION_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Create Vector Store\n",
    "Here, we use the `FAISS` library to create a vector store for retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Combine each question with its corresponding answer into a single string\n",
    "combined_texts = []\n",
    "for data in training_dataset:\n",
    "    combined_text = f\"question: {data['prompt']}\\n answer: {data['response']}\"\n",
    "    combined_texts.append(combined_text)\n",
    "\n",
    "split_docs = []\n",
    "for combined_text in combined_texts:\n",
    "    split_docs.extend(text_splitter.split_documents([Document(page_content= combined_text)]))  # Split the combined question-answer text\n",
    "\n",
    "# Create FAISS vector store from the split documents\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine-Tune the Model\n",
    "We fine-tune the LLaMA3.2 model using following methods.\n",
    "1. **LoRA**: Lightweight fine-tuning for adapters.\n",
    "2. **Full Fine-Tuning**: Training all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"...\"\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = hf_token\n",
    "\n",
    "\n",
    "def fine_tune_model(\n",
    "    training_file, \n",
    "    output_dir, \n",
    "    method=\"LoRA\", \n",
    "    model_name=\"EleutherAI/gpt-neo-125m\",  # More capable model\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced fine-tuning function with improved model and training strategies\n",
    "    \n",
    "    Args:\n",
    "    - training_file: Path to JSON training data\n",
    "    - output_dir: Directory to save fine-tuned model\n",
    "    - method: Fine-tuning method (default: LoRA)\n",
    "    - model_name: Base model to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load tokenizer and model with improved configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "    )    \n",
    "    # Set padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare model for efficient training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # LoRA Configuration with enhanced parameters\n",
    "    if method == \"LoRA\":\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # Increased rank for more capacity\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"q_proj\", \"v_proj\"]  # Target specific attention modules\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Enhanced preprocessing function\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"\n",
    "        Comprehensive preprocessing with enhanced formatting\n",
    "        \"\"\"\n",
    "        inputs = [\n",
    "            f\"Harry Potter Knowledge Base\\n\"\n",
    "            f\"Task: Provide an accurate and concise answer.\\n\"\n",
    "            f\"Q: {preprocess_text(question)}\\n\"\n",
    "            f\"A: {preprocess_text(answer)}{tokenizer.eos_token}\"\n",
    "            for question, answer in zip(examples['question'], examples['answer'])\n",
    "        ]\n",
    "        print(inputs[0])\n",
    "        \n",
    "        # Advanced tokenization\n",
    "        tokenized_inputs = tokenizer(\n",
    "            inputs, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add labels for language modeling loss\n",
    "        tokenized_inputs['labels'] = tokenized_inputs['input_ids'].clone()\n",
    "        \n",
    "        return tokenized_inputs\n",
    "    \n",
    "    # Load and preprocess dataset\n",
    "    dataset = load_dataset(\"json\", data_files=training_file)\n",
    "    split_data = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "    split_data = {\n",
    "        'train': split_data['train'],\n",
    "        'validation': split_data['test']\n",
    "    }\n",
    "    \n",
    "    tokenized_data = DatasetDict(split_data)\n",
    "\n",
    "    tokenized_data = tokenized_data.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=['question', 'answer']\n",
    "    )\n",
    "\n",
    "    eval_dataset = tokenized_data.get('validation')  # Check if this is None\n",
    "    if eval_dataset is None:\n",
    "        eval_dataset = tokenized_data.get('test')  # Fallback to another key\n",
    "        print(\"Using 'test' as eval_dataset:\", eval_dataset)\n",
    "    \n",
    "    # Prepare training arguments with advanced configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=5,  # Increased epochs\n",
    "        per_device_train_batch_size=4,  # Slightly increased batch size\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-5,  # Fine-tuned learning rate\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer with additional configurations\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data['train'],\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Helper function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing\n",
    "    \"\"\"\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "        \"LoRA\": {\n",
    "            \"output_dir\": os.path.join(OUTPUT_DIR, \"lora_fine_tuned_model_new\"),\n",
    "            \"method\": \"LoRA\",\n",
    "        },\n",
    "\n",
    "        # \"PEFT\": {\n",
    "        #     \"output_dir\": os.path.join(OUTPUT_DIR, \"peft_fine_tuned_model\"),\n",
    "        #     \"method\": \"peft\",\n",
    "        #     \"peft_config\": {\"task_type\": \"seq2seq\", \"adapter_hidden_size\": 64},\n",
    "        # },\n",
    "        # \"Full\": {\n",
    "        #     \"output_dir\": os.path.join(OUTPUT_DIR, \"full_fine_tuned_model\"),\n",
    "        #     \"method\": \"full\",\n",
    "        # }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "for method_name, params in methods.items():\n",
    "    model = fine_tune_model(training_file=TRAINING_DATA_PATH, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the Fine-Tuned Models\n",
    "We evaluate each fine-tuned model and compare the results using metrics such as BLEU, ROUGE, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "def evaluate_model(\n",
    "    model_path: str, \n",
    "    evaluation_file: str, \n",
    "    model_type: str = \"LoRA\", \n",
    "    evaluation_metrics: Optional[List[str]] = None,\n",
    "    max_eval_samples: Optional[int] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Comprehensively evaluate a fine-tuned model with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the fine-tuned model directory\n",
    "        evaluation_file (str): Path to the JSON evaluation dataset\n",
    "        model_type (str): Type of model loading ('LoRA' or 'Full')\n",
    "        evaluation_metrics (List[str], optional): Metrics to compute\n",
    "        max_eval_samples (int, optional): Limit number of evaluation samples\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, float]: Computed evaluation metrics\n",
    "    \"\"\"\n",
    "    # Default evaluation metrics if not specified\n",
    "    if evaluation_metrics is None:\n",
    "        evaluation_metrics = [\n",
    "            'exact_match', \n",
    "            'f1_score', \n",
    "            'bleu', \n",
    "            'rouge', \n",
    "            'meteor'\n",
    "        ]\n",
    "    \n",
    "    # Determine the best available device\n",
    "    device = torch.device(\n",
    "        \"mps\" if torch.backends.mps.is_available() \n",
    "        else \"cuda\" if torch.cuda.is_available() \n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        # Load base model configuration\n",
    "        if model_type == \"LoRA\":\n",
    "            try:\n",
    "                config = PeftConfig.from_pretrained(model_path)\n",
    "                base_model_name = config.base_model_name_or_path\n",
    "            except Exception as config_e:\n",
    "                print(f\"Error loading PEFT config: {config_e}\")\n",
    "                base_model_name = \"EleutherAI/gpt-neo-125m\"  # Fallback to default\n",
    "        else:\n",
    "            base_model_name = model_path\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": device}\n",
    "        )\n",
    "        \n",
    "        # Load and prepare model\n",
    "        if model_type == \"LoRA\":\n",
    "            # Try to load as PEFT model\n",
    "            try:\n",
    "                model = PeftModel.from_pretrained(\n",
    "                    base_model, \n",
    "                    model_path,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map={\"\": device}\n",
    "                )\n",
    "                \n",
    "                # Attempt to merge and unload LoRA weights\n",
    "                try:\n",
    "                    model = model.merge_and_unload()\n",
    "                except Exception as merge_e:\n",
    "                    print(f\"Warning: Could not merge LoRA weights: {merge_e}\")\n",
    "                    # Fallback to using the PEFT model without merging\n",
    "            except Exception as peft_e:\n",
    "                print(f\"Error loading PEFT model: {peft_e}\")\n",
    "                # Fallback to base model with LoRA config\n",
    "                from peft import LoraConfig, get_peft_model\n",
    "                lora_config = LoraConfig(\n",
    "                    r=16,\n",
    "                    lora_alpha=32,\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"CAUSAL_LM\",\n",
    "                    target_modules=[\"q_proj\", \"v_proj\"]\n",
    "                )\n",
    "                model = get_peft_model(base_model, lora_config)\n",
    "        else:\n",
    "            # Full fine-tuning model\n",
    "            model = base_model\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        model.to(device)\n",
    "        \n",
    "        # Verify model can generate\n",
    "        print(\"Checking model generation capability...\")\n",
    "        test_input = tokenizer(\"Test input\", return_tensors=\"pt\").to(device)\n",
    "        try:\n",
    "            _ = model.generate(**test_input, max_length=10)\n",
    "            print(\"Model generation verified successfully.\")\n",
    "        except Exception as gen_e:\n",
    "            print(f\"Generation test failed: {gen_e}\")\n",
    "            raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Comprehensive model loading error: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Load evaluation dataset\n",
    "    try:\n",
    "        eval_dataset = load_dataset(\"json\", data_files=evaluation_file)['train']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation dataset: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Limit evaluation samples if specified\n",
    "    if max_eval_samples:\n",
    "        eval_dataset = eval_dataset.select(range(min(len(eval_dataset), max_eval_samples)))\n",
    "    \n",
    "    # Initialize metric objects\n",
    "    metric_loaders = {\n",
    "        'exact_match': evaluate.load(\"exact_match\"),\n",
    "        'f1_score': evaluate.load(\"f1\"),\n",
    "        'bleu': evaluate.load(\"bleu\"),\n",
    "        'rouge': evaluate.load(\"rouge\"),\n",
    "        'meteor': evaluate.load(\"meteor\")\n",
    "    }\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # Enhanced generation function\n",
    "    def generate_response(input_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate model response with improved generation parameters.\n",
    "        \n",
    "        Args:\n",
    "            input_text (str): Input question to generate response for\n",
    "        \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        # Prepare input with context formatting\n",
    "        formatted_input = (\n",
    "            f\"Harry Potter Knowledge Base\\n\"\n",
    "            f\"Task: Provide an accurate and concise answer.\\n\"\n",
    "            f\"{input_text}\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            formatted_input, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response with advanced parameters\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=512, \n",
    "                    num_return_sequences=1, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    temperature=0.7,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating response: {e}\")\n",
    "                return \"\"\n",
    "        \n",
    "        # Decode and clean response\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Evaluation loop\n",
    "    for item in eval_dataset:\n",
    "        # Generate prediction\n",
    "        pred = generate_response(f\"Q: {item['question']}\")\n",
    "        ref = item['answer']\n",
    "        \n",
    "        # Optional verbose logging\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"Reference: {ref}\\n\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = {}\n",
    "    print(predictions)\n",
    "    for metric_name in evaluation_metrics:\n",
    "        try:\n",
    "            if metric_name == 'exact_match':\n",
    "                results['exact_match'] = metric_loaders['exact_match'].compute(\n",
    "                    predictions=predictions, \n",
    "                    references=references\n",
    "                )['exact_match']\n",
    "            \n",
    "            elif metric_name == 'f1_score':\n",
    "                results['f1_score'] = metric_loaders['f1_score'].compute(\n",
    "                    predictions=predictions, \n",
    "                    references=references, \n",
    "                    average='weighted'\n",
    "                )['f1']\n",
    "            \n",
    "            elif metric_name == 'bleu':\n",
    "                results['bleu'] = metric_loaders['bleu'].compute(\n",
    "                    predictions=predictions, \n",
    "                    references=references\n",
    "                )['bleu']\n",
    "            \n",
    "            elif metric_name == 'rouge':\n",
    "                rouge_results = metric_loaders['rouge'].compute(\n",
    "                    predictions=predictions, \n",
    "                    references=references\n",
    "                )\n",
    "                results['rouge'] = {\n",
    "                    'rouge1': rouge_results['rouge1'],\n",
    "                    'rouge2': rouge_results['rouge2'],\n",
    "                    'rougeL': rouge_results['rougeL']\n",
    "                }\n",
    "            \n",
    "            elif metric_name == 'meteor':\n",
    "                results['meteor'] = metric_loaders['meteor'].compute(\n",
    "                    predictions=predictions, \n",
    "                    references=references\n",
    "                )['meteor']\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing {metric_name} metric: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LoRA\": os.path.join(OUTPUT_DIR, \"lora_fine_tuned_model_new\"),\n",
    "    #\"PEFT\": os.path.join(OUTPUT_DIR, \"peft_fine_tuned_model\"),\n",
    "    #\"Full\": os.path.join(OUTPUT_DIR, \"full_fine_tuned_model\")\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "for method_name, model_path in models.items():\n",
    "    evaluation_results[method_name] = evaluate_model(\n",
    "        model_path=model_path,\n",
    "        evaluation_file=EVALUATION_DATA_PATH,\n",
    "        model_type=method_name,    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results[\"LoRA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_llm = ChatOllama(model=model, n_ctx=2048, temperature=0.7, device=\"cpu\")\n",
    "rag_qa_chain = RetrievalQA.from_chain_type(llm=rag_llm, retriever=retriever, return_source_documents=True)\n",
    "evaluation_results[\"RAG\"] = evaluate_model(rag_qa_chain, evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Visualize Results\n",
    "Finally, we display the evaluation metrics in a clear and concise format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, metrics in evaluation_results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\" if isinstance(value, (int, float)) else f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example visualization\n",
    "metrics = [\"exact_match\", \"f1_score\", \"bleu_score\"]\n",
    "for metric in metrics:\n",
    "    plt.bar(evaluation_results.keys(), [evaluation_results[method][metric] for method in methods])\n",
    "    plt.title(f\"Comparison of {metric} Scores\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Sample Question-Answer Showcase\n",
    "To illustrate the differences between the models, we test them on the same sample questions and compare their responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\n",
    "    \"Who are friends of Harry potter?\",\n",
    "    \"Who wrote 'Harry Potter'?\",\n",
    "    \"who are Ron Weasely's parents?\",\n",
    "]\n",
    "\n",
    "# Generate answers from each model\n",
    "for method, model_path in models.items():\n",
    "    print(f\"\\n=== {method} Model Responses ===\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=model, retriever=retriever, return_source_documents=True)\n",
    "    \n",
    "    for question in sample_questions:\n",
    "        response = qa_chain.run(question)\n",
    "        print(f\"Q: {question}\\nA: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
